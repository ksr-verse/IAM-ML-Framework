{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IAM ML Framework - Example Analysis\n",
        "\n",
        "This notebook demonstrates how to use the IAM ML Framework for identity and access management data analysis.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. Loading and exploring IAM data\n",
        "2. Data preprocessing and feature engineering\n",
        "3. Training ML models for approval prediction\n",
        "4. Generating actionable insights\n",
        "5. Creating visualizations\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have:\n",
        "- Run `pip install -r requirements.txt`\n",
        "- Generated sample data with `python data/generate_dummy_data.py`\n",
        "- Or configured your MySQL connection in `config/db_config.yaml`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "from database import DatabaseConnector\n",
        "from preprocessing import DataPreprocessor\n",
        "from model_training import ModelTrainer\n",
        "from insights import InsightsGenerator\n",
        "from visualization import Visualizer\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data\n",
        "\n",
        "The framework automatically loads data from MySQL or CSV files based on your configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize database connector\n",
        "db = DatabaseConnector(\n",
        "    db_config_path='../config/db_config.yaml',\n",
        "    schema_config_path='../config/schema_config.yaml'\n",
        ")\n",
        "\n",
        "# Connect and load tables\n",
        "db.connect()\n",
        "tables = db.fetch_all_tables()\n",
        "\n",
        "print(f\"\\nLoaded {len(tables)} tables:\")\n",
        "for table_name, df in tables.items():\n",
        "    print(f\"  - {table_name}: {len(df)} rows, {len(df.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Explore the Data\n",
        "\n",
        "Let's examine each dataset to understand the IAM data structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision History - tracks approval/rejection decisions\n",
        "print(\"=== Decision History ===\")\n",
        "decision_history = tables['decision_history']\n",
        "print(f\"Shape: {decision_history.shape}\")\n",
        "print(f\"\\nColumns: {list(decision_history.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "display(decision_history.head())\n",
        "\n",
        "print(f\"\\nDecision distribution:\")\n",
        "print(decision_history['decision'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preprocess Data\n",
        "\n",
        "The framework handles cleaning, merging, and encoding automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get configuration\n",
        "ml_config = db.get_ml_config()\n",
        "merge_strategy = db.get_merge_strategy()\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = DataPreprocessor(ml_config, merge_strategy)\n",
        "\n",
        "# Run preprocessing pipeline\n",
        "features, target = preprocessor.process_pipeline(tables)\n",
        "\n",
        "print(f\"\\n=== Preprocessing Results ===\")\n",
        "print(f\"Features shape: {features.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")\n",
        "print(f\"\\nFeature columns: {list(features.columns)[:10]}...\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(target.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train ML Models\n",
        "\n",
        "Train multiple classification models to predict approval/rejection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = ModelTrainer(ml_config, models_dir='../models')\n",
        "\n",
        "# Train models\n",
        "results = trainer.train(features, target)\n",
        "\n",
        "print(\"\\n=== Training Results ===\")\n",
        "print(f\"\\nModels trained: {list(results['models'].keys())}\")\n",
        "\n",
        "# Display metrics\n",
        "for model_name, metrics in results['metrics'].items():\n",
        "    print(f\"\\n{model_name.upper()}:\")\n",
        "    print(f\"  Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
        "    print(f\"  F1-Score: {metrics.get('f1_score', 0):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Insights\n",
        "\n",
        "Extract actionable insights for IAM operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get original data for context\n",
        "cleaned_tables = {name: preprocessor.clean_dataframe(df, name) \n",
        "                 for name, df in tables.items()}\n",
        "original_data = preprocessor.merge_tables(cleaned_tables)\n",
        "\n",
        "# Generate insights\n",
        "insights_config = db.get_insights_config()\n",
        "insights_gen = InsightsGenerator(insights_config, models_dir='../models', output_dir='../outputs/insights')\n",
        "\n",
        "insights = insights_gen.generate_all_insights(features, target, original_data)\n",
        "\n",
        "print(\"\\n=== Insights Generated ===\")\n",
        "for key in insights.keys():\n",
        "    print(f\"  - {key}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: View Insights Report\n",
        "\n",
        "Generate comprehensive text report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and display report\n",
        "report = insights_gen.generate_report()\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Create Visualizations\n",
        "\n",
        "Generate comprehensive visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "viz = Visualizer(output_dir='../outputs/visualizations')\n",
        "viz.generate_all_visualizations(features, target, original_data, insights)\n",
        "\n",
        "print(f\"\\nVisualizations saved to: {viz.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Examples\n",
        "\n",
        "Let's display some key visualizations inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Risk score distribution by decision\n",
        "if 'risk_score' in original_data.columns and 'decision' in original_data.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    for decision in original_data['decision'].unique():\n",
        "        data = original_data[original_data['decision'] == decision]['risk_score']\n",
        "        plt.hist(data, alpha=0.5, label=decision, bins=30)\n",
        "    \n",
        "    plt.xlabel('Risk Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Risk Score Distribution by Decision')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Data Loading**: Automatically fetch data from MySQL or CSV files\n",
        "2. **Preprocessing**: Clean, merge, and encode IAM data\n",
        "3. **ML Training**: Build classification models for approval prediction\n",
        "4. **Feature Importance**: Understand key decision drivers\n",
        "5. **Insights**: Identify access reduction opportunities and risk trends\n",
        "6. **Visualizations**: Create actionable dashboards\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- **Customize**: Edit `config/schema_config.yaml` to define your own tables\n",
        "- **Extend**: Add new models in `src/model_training.py`\n",
        "- **Deploy**: Use trained models for real-time predictions\n",
        "- **Automate**: Run `main.py` for complete pipeline execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disconnect from database\n",
        "db.disconnect()\n",
        "print(\"Analysis complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
